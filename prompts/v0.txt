-------------------- PROMPT 1 --------------------

Alright, this guy have created this GitHub repo detailing 500 production issues in k8s, and itâ€™s format is like this.

Now, based on that I want to create a website where people can go through them in easily readable manner as repo is not. 

Iâ€™m think of card per scenario, and then when I click on card, it sends me to detailed page.

Also, add credit to this guy -> Vijay2181 https://github.com/vijay2181/k8s-500-prod-issues

<format>
ðŸ“˜ Scenario #1: Zombie Pods Causing NodeDrain to Hang
Category: Cluster Management
Environment: K8s v1.23, On-prem bare metal, Systemd cgroups
Scenario Summary: Node drain stuck indefinitely due to unresponsive terminating pod.
What Happened: A pod with a custom finalizer never completed termination, blocking kubectl drain. Even after the pod was marked for deletion, the API server kept waiting because the finalizer wasnâ€™t removed.
Diagnosis Steps:
	â€¢ Checked kubectl get pods --all-namespaces -o wide to find lingering pods.
	â€¢ Found pod stuck in Terminating state for over 20 minutes.
	â€¢ Used kubectl describe pod <pod> to identify the presence of a custom finalizer.
	â€¢ Investigated controller logs managing the finalizer â€“ the controller had crashed.
Root Cause: Finalizer logic was never executed because its controller was down, leaving the pod undeletable.
Fix/Workaround:
kubectl patch pod <pod-name> -p '{"metadata":{"finalizers":[]}}' --type=merge
Lessons Learned: Finalizers should have timeout or fail-safe logic.
How to Avoid:
	â€¢ Avoid finalizers unless absolutely necessary.
	â€¢ Add monitoring for stuck Terminating pods.
	â€¢ Implement retry/timeout logic in finalizer controllers.

ðŸ“˜ Scenario #2: API Server Crash Due to Excessive CRD Writes
Category: Cluster Management
Environment: K8s v1.24, GKE, heavy use of custom controllers
Scenario Summary: API server crashed due to flooding by a malfunctioning controller creating too many custom resources.
What Happened: A bug in a controller created thousands of Custom Resources (CRs) in a tight reconciliation loop. Etcd was flooded, leading to slow writes, and the API server eventually became non-responsive.
Diagnosis Steps:
	â€¢ API latency increased, leading to 504 Gateway Timeout errors in kubectl.
	â€¢ Used kubectl get crds | wc -l to list all CRs.
	â€¢ Analyzed controller logs â€“ found infinite reconcile on a specific CR type.
	â€¢ etcd disk I/O was maxed.
Root Cause: Bad logic in reconcile loop: create was always called regardless of the state, creating resource floods.
Fix/Workaround:
	â€¢ Scaled the controller to 0 replicas.
	â€¢ Manually deleted thousands of stale CRs using batch deletion.
Lessons Learned: Always test reconcile logic in a sandboxed cluster.
How to Avoid:
	â€¢ Implement create/update guards in reconciliation.
	â€¢ Add Prometheus alert for high CR count.
</format>

-------------------- PROMPT 2 --------------------
Alright this look good, please make theme it similar to Kubernetes project and Kubernets branding. Make the website fun to interact and also easier to read with great font style, enough spacing, professional layout of front page and details page that is easier to read, similar to how Ruby on Rails projects are. Also, mention me (Pur Tuladhar) who created this, and with LinkedIn.
